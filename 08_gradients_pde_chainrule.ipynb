{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d7aead",
   "metadata": {},
   "source": [
    "# Chapter 8: Gradients, Partial Derivatives, and the Chain Rule\n",
    "\n",
    "Our neural network consists of neurons, which have multiple inputs, each input gets multiplied by the corresponding weight,and they get summed with the bias.\n",
    "\n",
    "To learn the impact of all of the inputs, weights, and biases to the neuron output and the loss function, we need to calculate the derivative of each operation performed during the forward pass in the neuron and the whole model.\n",
    "\n",
    "To do that we need to use the chain rule.\n",
    "\n",
    "## 8.1. The Partial Derivative\n",
    "\n",
    "The partial derivative measures how much impact a single input has on a function’s output.\n",
    "\n",
    "Euler’s notation $∂$ is used instead of Leibniz’s notation $d$.\n",
    "\n",
    "$$\n",
    "f(x,y,z) \\quad \\to \\quad \\frac{∂}{∂x} f(x,y,z), \\frac{∂}{∂y} f(x,y,z), \\frac{∂}{∂z} f(x,y,z)\n",
    "$$\n",
    "\n",
    "The gradient is a vector of the size of inputs containing partial derivative solutions with respect to each of the inputs.\n",
    "\n",
    "## 8.2. The Partial Derivative of a Sum\n",
    "\n",
    "Calculating the partial derivative with respect to a given input means to calculate it like the regular derivative of one input, and treat other inputs as constants.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x,y) = x + y \\quad \\to \\quad & \\frac{∂}{∂x} f(x,y) = \\frac{∂}{∂x} \\big[ x+y \\big] = \\frac{∂}{∂x} x + \\frac{∂}{∂x} y = 1 + 0 = 1 \\\\\n",
    "& \\frac{∂}{∂y} f(x,y) = \\frac{∂}{∂y} \\big[ x+y \\big] = \\frac{∂}{∂y} x + \\frac{∂}{∂y} y = 0 + 1 = 1 \\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x,y) = 2x + 3y^2 \\quad \\to \\quad \\frac{∂}{∂x} f(x,y) & = \\frac{∂}{∂x} \\big[ 2x + 3y^2 \\big] = \\frac{∂}{∂x} 2x + \\frac{∂}{∂x} 3y^2 \\\\\n",
    "& = 2 \\cdot \\frac{∂}{∂x} x + 3 \\cdot \\frac{∂}{∂x} y^2 = 2 \\cdot 1 + 3 \\cdot 0 = 2 \\\\\n",
    "\\frac{∂}{∂y} f(x,y) & = \\frac{∂}{∂y} \\big[ 2x + 3y^2 \\big] = \\frac{∂}{∂y} 2x + \\frac{∂}{∂y} 3y^2 \\\\\n",
    "& =  2 \\cdot \\frac{∂}{∂y} x + 3 \\cdot \\frac{∂}{∂y} y^2 = 2 \\cdot 0 + 3 \\cdot 2 y^1 = 6y \\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x,y)  = 3x^3 - y^2 + 5x + 2 \\quad & \\to \\\\\n",
    "\\frac{∂}{∂x} f(x,y) & = \\frac{∂}{∂x} \\big[ 3x^3 - y^2 + 5x + 2 \\big] = \\frac{∂}{∂x} 3x^3 - \\frac{∂}{∂x} y^2 + \\frac{∂}{∂x} 5x + \\frac{∂}{∂x} 2 \\\\\n",
    "& = 3 \\cdot \\frac{∂}{∂x} x^3 - \\frac{∂}{∂x} y^2 + 5 \\cdot \\frac{∂}{∂x} x + \\frac{∂}{∂x} 2 = 3 \\cdot 3x^2 - 0 + 5 \\cdot 1 + 0 = 9x^2 +5\\\\\n",
    "\\frac{∂}{∂y} f(x,y) & = \\frac{∂}{∂y} \\big[ 3x^3 - y^2 + 5x + 2 \\big] = \\frac{∂}{∂y} 3x^3 - \\frac{∂}{∂y} y^2 + \\frac{∂}{∂y} 5x + \\frac{∂}{∂y} 2  \\\\\n",
    "& = 3 \\cdot \\frac{∂}{∂y} x^3 - \\frac{∂}{∂y} y^2 + 5 \\cdot \\frac{∂}{∂y} x + \\frac{∂}{∂y} 2 = 3 \\cdot 0 - 2 y^1 + 5 \\cdot 0 + 0 = -2y  \\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "## 8.3. The Partial Derivative of Multiplication\n",
    "\n",
    "We need to treat the other independent variables as constants, so we can move constants to the outside of the derivative.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x,y) = x \\cdot y \\quad \\to \\quad & \\frac{∂}{∂x} f(x,y) = \\frac{∂}{∂x} \\big[ x \\cdot y \\big] = y \\frac{∂}{∂x} x  = y \\cdot 1 = y \\\\\n",
    "& \\frac{∂}{∂y} f(x,y) = \\frac{∂}{∂y} \\big[ x \\cdot y \\big] = x \\frac{∂}{∂y} y = x \\cdot 1 = x\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let’s introduce a third input variable and add multiplication of variables for another example:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x,y, z)  = 3x^3z - y^2 + 5z + 2yz \\quad & \\to \\\\\n",
    "\\frac{∂}{∂x} f(x,y, z) & = \\frac{∂}{∂x} \\big[ 3x^3z - y^2 + 5z + 2yz \\big] \\\\\n",
    "& = \\frac{∂}{∂x} 3x^3z - \\frac{∂}{∂x} y^2 + \\frac{∂}{∂x} 5z  + \\frac{∂}{∂x} 2yz \\\\\n",
    "& = 3z \\cdot \\frac{∂}{∂x} x^3 - \\frac{∂}{∂x} y^2 + 5 \\cdot \\frac{∂}{∂x}5z  + 2 \\cdot \\frac{∂}{∂x} yz \\\\\n",
    "& = 3z \\cdot 3x^2 - 0 + 5 \\cdot 0 + 2 \\cdot 0 = 9x^2 z  \\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x,y, z)  = 3x^3z - y^2 + 5z + 2yz \\quad & \\to \\\\\n",
    "\\frac{∂}{∂y} f(x,y, z) & = \\frac{∂}{∂y} \\big[ 3x^3z - y^2 + 5z + 2yz \\big] \\\\\n",
    "& = \\frac{∂}{∂y} 3x^3z - \\frac{∂}{∂y} y^2 + \\frac{∂}{∂y} 5z  + \\frac{∂}{∂y} 2yz \\\\\n",
    "& = 3 \\cdot \\frac{∂}{∂y} x^3z - \\frac{∂}{∂y} y^2 + 5 \\cdot \\frac{∂}{∂y}z  + 2z \\cdot \\frac{∂}{∂y} y \\\\\n",
    "& = 3 \\cdot 0 - 2y + 5 \\cdot 0 + 2z \\cdot 1 = -2y + 2z \\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x,y, z)  = 3x^3z - y^2 + 5z + 2yz \\quad & \\to \\\\\n",
    "\\frac{∂}{∂z} f(x,y, z) & = \\frac{∂}{∂z} \\big[ 3x^3z - y^2 + 5z + 2yz \\big] \\\\\n",
    "& = \\frac{∂}{∂z} 3x^3z - \\frac{∂}{∂z} y^2 + \\frac{∂}{∂z} 5z  + \\frac{∂}{∂z} 2yz \\\\\n",
    "& = 3x^3 \\cdot \\frac{∂}{∂z} z - \\frac{∂}{∂z} y^2 + 5 \\cdot \\frac{∂}{∂z} z  + 2y \\cdot \\frac{∂}{∂z} z  \\\\\n",
    "& = 3x^3 \\cdot 1 - 0 + 5 \\cdot 1 + 2y \\cdot 1 = 3x^3 + 5 + 2y \\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## 8.4. The Partial Derivative of Max\n",
    "\n",
    "The max function returns the greatest input.\n",
    "\n",
    "$$\n",
    "f(x,y) = max(x,y) \\quad \\to \\quad \\frac{∂}{∂x} f(x,y) = \\frac{∂}{∂x} max(x,y) = 1 (x > y)\n",
    "$$\n",
    "\n",
    "If $x$ is greater than $y$, the derivative of $f(x,y)$ with respect to $x$ equals 1.\n",
    "\n",
    "If $y$ is greater than $x$, the derivative of $f(x,y)$ with respect to $x$ equals 0 — we treat $y$ as a constant.\n",
    "\n",
    "The ReLU activation function effectively clips the input value at 0 from the positive side.\n",
    "\n",
    "$$\n",
    "f(x) = max(x,0) \\quad \\to \\quad \\frac{d}{dx} f(x) = \\frac{d}{dx} max(x,0) = 1 (x > 0)\n",
    "$$\n",
    "\n",
    "We used $d$ instead of $∂$ as the function takes a single parameter, we calculate the non-partial derivative.\n",
    "\n",
    "\n",
    "## 8.5. The Gradient\n",
    "\n",
    "The gradient is a vector composed of all of the partial derivatives of a function, calculated with respect to each input variable.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x, y, z)  = 3x^3z - y^2 + 5z + 2yz \\quad & \\to \\\\\n",
    "\\frac{∂}{∂x} f(x,y, z) & = 9x^2z \\\\\n",
    "\\frac{∂}{∂y} f(x,y, z) & = -2y + 2z  \\\\\n",
    "\\frac{∂}{∂z} f(x,y, z) & = 3x^3 + 5 + 2y \\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The gradient of the function is denoted using `nabla` symbol $∇$ that looks like an inverted delta symbol.\n",
    "\n",
    "$$\n",
    "\\nabla f(x, y, z) = \\begin{bmatrix}\n",
    "\\frac{∂}{∂x} f(x,y, z)  \\\\\n",
    "\\frac{∂}{∂y} f(x,y, z) \\\\\n",
    "\\frac{∂}{∂z} f(x,y, z)\n",
    "\\end{bmatrix} =  \\begin{bmatrix}\n",
    "\\frac{∂}{∂x}  \\\\\n",
    "\\frac{∂}{∂y} \\\\\n",
    "\\frac{∂}{∂z}\n",
    "\\end{bmatrix} f(x, y, z) = \\begin{bmatrix}\n",
    "9x^2z  \\\\\n",
    "-2y + 2z \\\\\n",
    "3x^3 + 5 + 2y\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We will perform the gradient descent using the chain rule to perform the backward pass, as a part of the model training.\n",
    "\n",
    "\n",
    "## 8.6. The Chain Rule\n",
    "\n",
    "The forward pass through two consecutive neurons can be described as\n",
    "\n",
    "$$\n",
    "z = f(x)\\\\\n",
    "y = g(z)\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "y = g\\big(  f(x) \\big)\n",
    "$$\n",
    "\n",
    "The output of $g$ is influenced by $x$ in some way. So, there must exist a derivative.\n",
    "\n",
    "The loss function takes output, targets, samples, weights, and biases as input parameters.\n",
    "\n",
    "<center><img src='./image/1-11-loss-function.png' style='width: 60%'/></center>\n",
    "\n",
    "<center><img src='./image/1-12-loss-function-code.png' style='width: 60%'/><font color='gray'><i>Code for a forward pass of an example neural network model.</i></font></center>\n",
    "\n",
    "To improve loss, we need to learn how each weight and bias impacts it.\n",
    "\n",
    "The chain rule turns is the most important rule in finding the impact of singular input to the output of a chain of functions.\n",
    "\n",
    "The chain rule says that the derivative of a function chain is a product of derivatives of all of the functions in this chain.\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} f \\big( g(x) \\big) = \\frac{d f \\big( g(x) \\big)}{dg(x)} \\cdot \\frac{dg(x)}{dx} = f' \\big( g(x) \\big) \\cdot g'(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{∂}{∂x} f \\Big( g \\big( y, h(x,z)  \\big) \\Big) = \\frac{∂ f \\Big( g \\big( y, h(x,z)  \\big) \\Big)}{∂ g \\big( y, h(x,z)  \\big) } \\cdot \\frac{∂ g \\big( y, h(x,z)  \\big)}{∂ h(x,z) } \\cdot \\frac{∂ h(x,z)}{∂x}\n",
    "$$\n",
    "\n",
    "Example of applying chain rule:\n",
    "\n",
    "$$\n",
    "h(x) = f \\big( g(x) \\big) = 3(2x^2)^5 \\quad \\to \\quad f' \\big( g(x)  \\big) = 3 \\cdot 5 (2 x^2)^{5-1} = 15 (2x^2)^4\\\\\n",
    "\\begin{align}\n",
    "\\to \\quad h'(x) & = f' \\big( g(x) \\big) \\cdot g'(x) = 15(2x^2)^4 \\cdot \\frac{d}{dx} 2x^2 = 15(2x^2)^4 \\cdot 2 \\cdot \\frac{d}{dx} x^2 \\\\\n",
    "& = 15(2x^2)^4 \\cdot 2 \\cdot 2x^1 = 15(2x^2)^4 \\cdot 4x \\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We usually do:\n",
    "\n",
    "$$\n",
    "f(x) = 3 \\big(  2x^2 \\big)^5 \\quad \\to \\quad f'(x) = 15(2x^2)^4 \\cdot 4x\\\\\n",
    "f'(x) = 15 \\cdot 2^4 \\cdot 4 \\cdot x^9 = 960 x^9\n",
    "$$\n",
    "\n",
    "\n",
    "## 8.7. Summary\n",
    "\n",
    "The partial derivative of the sum with respect to any input equals 1:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x, y)  = x+y \\quad \\to \\quad & \\frac{∂}{∂x} f(x, y) = 1\\\\\n",
    "& \\frac{∂}{∂y} f(x, y) = 1 \\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The partial derivative of the multiplication operation with 2 inputs, with respect to any input, equals the other input:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x, y)  = x \\cdot y \\quad \\to \\quad & \\frac{∂}{∂x} f(x, y) = y \\\\\n",
    "& \\frac{∂}{∂y} f(x, y) = x \\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The partial derivative of the max function of 2 variables with respect to any of them is 1 if this variable is the biggest and 0 otherwise.\n",
    "\n",
    "$$\n",
    "f(x, y) = max(x,y) \\quad \\to \\quad  \\frac{∂}{∂x} f(x, y) =  1(x >y)\n",
    "$$\n",
    "\n",
    "The derivative of the max function of a single variable and 0 equals 1 if the variable is greater than 0 and 0 otherwise:\n",
    "\n",
    "$$\n",
    "f(x) = max(x,0) \\quad \\to \\quad  \\frac{∂}{∂x} f(x) =  1(x >0)\n",
    "$$\n",
    "\n",
    "The derivative of chained functions equals the product of the partial derivatives of the subsequent functions:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} f \\big( g(x) \\big) = \\frac{d}{dg(x)} f \\big( g(x) \\big) \\cdot \\frac{d}{dx} g(x) = f' \\big( g(x) \\big) \\cdot g'(x)\n",
    "$$\n",
    "\n",
    "The same applies to the partial derivatives.\n",
    "\n",
    "$$\n",
    "\\frac{∂}{∂x} f\\Big(  g \\big( y, h(x,z) \\big) \\Big) = f' \\Big(  g \\big( y, h(x,z) \\big) \\Big) \\cdot g' \\big( y, h(x,z) \\big) \\cdot h'(x,z)\n",
    "$$\n",
    "\n",
    "The gradient is a vector of all possible partial derivatives. An example of a triple-input function:\n",
    "\n",
    "$$\n",
    "\\nabla f(x,y,z) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{∂}{∂x} f(x,y,z) \\\\\n",
    "\\frac{∂}{∂y} f(x,y,z) \\\\\n",
    "\\frac{∂}{∂z} f(x,y,z)\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{∂}{∂x} \\\\\n",
    "\\frac{∂}{∂y} \\\\\n",
    "\\frac{∂}{∂z}\n",
    "\\end{bmatrix} f(x,y,z)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ed8138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnfs",
   "language": "python",
   "name": "nnfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
