{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "356ef51c",
   "metadata": {},
   "source": [
    "# Chapter 4: Activation Functions\n",
    "\n",
    "The activation function mimic a neuron “firing” or “not firing” based on input information.\n",
    "There are two types of activation functions:\n",
    "\n",
    "- The activation functions used in hidden layers (usually will be the same for all neurons, but doesn’t have to).\n",
    "- The activation functions used in the output layer.\n",
    "\n",
    "## 4.1. The Step Activation Function\n",
    "\n",
    "It is rarely a choice nowadays as it is not very informative. The return value does not contain all information about the input (an input of 3 will output the same value as an input of 3000).\n",
    "\n",
    "<center><img src='./image/4-1.png' style='width: 60%'/></center>\n",
    "\n",
    "## 4.2. The Linear Activation Function\n",
    "\n",
    "This activation function is usually applied to the last layer’s output in the case of a regression model.\n",
    "\n",
    "<center><img src='./image/4-2.png' style='width: 60%'/></center>\n",
    "\n",
    "## 4.3. The Sigmoid Activation Function\n",
    "\n",
    "This activation function is more granular than the step activation function.\n",
    "\n",
    "The return value contains all information about the input. This function works better with NNs.\n",
    "\n",
    "The Sigmoid function will be used as the output layer’s activation function.\n",
    "\n",
    "<center><img src='./image/4-3.png' style='width: 60%'/></center>\n",
    "\n",
    "The sigmoid function, historically used in hidden layers, was eventually repplaced by the Rectified Linear Units activation function (ReLU).\n",
    "\n",
    "## 4.4. The Rectified Linear Activation Function\n",
    "\n",
    "The most widely used activation function – mainly because of speed and efficiency.\n",
    "\n",
    "The Sigmoid activation function is not the most complicated, but it’s still more challenging to compute than the ReLU activation function.\n",
    "\n",
    "ReLU activation function is extremely close to being a linear activation function while remaining nonlinear, due to that bend after 0. This simple property is very effective.\n",
    "\n",
    "<center><img src='./image/4-4.png' style='width: 60%'/></center>\n",
    "\n",
    "## 4.5. Why Use Activation Function?\n",
    "\n",
    "NNs need to contain two or more hidden layers to fit a nonlinear function, we need those hidden layers to use a nonlinear activation function.\n",
    "\n",
    "A nonlinear function cannot be represented well by a straight line.\n",
    "\n",
    "An example of linear problem in life:\n",
    "\n",
    "- The cost of some number of shirts, know the cost of an individual shirt.\n",
    "\n",
    "An example of nonlinear problem in life:\n",
    "\n",
    "- The price of a home depends on size, location, time of year attempting to sell, number of rooms, yard, neighborhood, and so on.\n",
    "\n",
    "A neural network with 2 hidden layers of 8 neurons each, the result of training this model will look like:\n",
    "\n",
    "<center><img src='./image/4-5.png' style='width: 60%'/></center>\n",
    "\n",
    "A neural network with 2 hidden layers of 8 neurons each, using the linear activation function:\n",
    "\n",
    "<center><img src='./image/4-6.png' style='width: 60%'/></center>\n",
    "\n",
    "The same NN architecture, using the rectified linear activation function:\n",
    "\n",
    "<center><img src='./image/4-7.png' style='width: 60%'/></center>\n",
    "\n",
    "## 4.6. Linear Activation in the Hidden Layers\n",
    "\n",
    "Consider a NN with all linear activation functions of $y = x$.\n",
    "\n",
    "<center><img src='./image/4-8.png' style='width: 60%'/></center>\n",
    "\n",
    "No matter what we do, how many layers we have, this NN can only depict linear relationships if we use linear activation functions.\n",
    "\n",
    "The entire network is a linear function as well.\n",
    "\n",
    "## 4.7. ReLU Activation in a Pair of Neurons\n",
    "\n",
    "How the rectified linear activation function can suddenly map nonlinear relationships and functions?\n",
    "\n",
    "<center><img src='./image/4-9.png' style='width: 60%'/><font color='gray'><i>Single neuron with a weight of 0 and a bias of 0.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-10.png' style='width: 60%'/><font color='gray'><i>Single neuron with a weight of 1 and a bias of 0.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-11.png' style='width: 60%'/><font color='gray'><i>Single neuron with a weight of 1 and a bias of 0.5.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-12.png' style='width: 60%'/><font color='gray'><i>Single neuron with a weight of -1 and a bias of 0.5. (See when this neuron deactivates)</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-13.png' style='width: 60%'/><font color='gray'><i>Pair neurons, the 2nd neuron’s bias does no offsetting, output is linear.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-14.png' style='width: 60%'/><font color='gray'><i>Pair neurons, the 2nd neuron bias of 1 (shifts the overall function vertically).</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-15.png' style='width: 60%'/><font color='gray'><i>Pair neurons, the 2nd neuron weight of -2, have both an activation and a deactivation point. When both neurons are activated, “area of effect” comes into play, produce values in the range of the granular variable output. If any neuron in the pair is inactive, the pair will produce non-variable output.\n",
    "</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-16.png' style='width: 60%'/><font color='gray'><i>Pair neurons, the 2nd neuron weight of -2, “area of effect”.</i></font></center>\n",
    "\n",
    "## 4.8. ReLU Activation in the Hidden Layers\n",
    "\n",
    "We will fit the sine wave function using 2 hidden layers of 8 neurons each, and <font color='green'>hand-tune the values to fit the curve</font>.\n",
    "\n",
    "We do this by working with 1 pair of neurons at a time, which means 1 neuron from each layer individually.\n",
    "\n",
    "Assume that the layers are not densely connected, each neuron from the first hidden layer connects to only one neuron from the second hidden layer for simplification.\n",
    "\n",
    "The model takes 1 input ($x$) and return 1 output $y = sin(x)$.\n",
    "\n",
    "The output layer uses the Linear activation function.\n",
    "\n",
    "The hidden layers use the ReLU activation function.\n",
    "\n",
    "<center><img src='./image/4-17.png' style='width: 60%'/><font color='gray'><i>Start with the first pair of neurons, set all weights to 0.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-18.png' style='width: 60%'/><font color='gray'><i>Set weights of the hidden layer neurons and the output neuron to 1.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-19.png' style='width: 60%'/><font color='gray'><i>Adjust the weight for the first neuron of the first layer to 6.</i></font><font color='green'><i> The initial slope is correct.</i></font><font color='red'><i> A problem is this function never ends</i></font><font color='gray'><i> as this neuron pair never deactivates. We want the deactivation to occur where the red fitment line diverges initially from the green sine wave (~ 0.7).</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-20.png' style='width: 60%'/><font color='gray'><i>Increase the bias for the 2nd neuron to 0.7. This offsets the overall function vertically. </i></font></center>\n",
    "\n",
    "<center><img src='./image/4-21.png' style='width: 60%'/><font color='gray'><i>Set the weight for the 2nd neuron to -1, causing the deactivation point to occur where we want.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-22.png' style='width: 60%'/><font color='gray'><i>Flip this slope back by seting the weight of the connection to the output neuron to -1. Now we need to offset this up a bit.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-23.png' style='width: 60%'/><font color='gray'><i>We will use first 7-pairs of neurons in the hidden layers to create the sine wave’s shape, </i></font><font color='green'><i>then the bottom pair to offset everything vertically.</i></font><font color='gray'><i> Now we completed the first section.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-24.png' style='width: 60%'/><font color='gray'><i>Set all weights for this 2nd pair of neurons to 1, including the output neuron.</i></font><font color='red'><i> The 2nd pair begins activation too soon</i></font><font color='gray'><i>, which impacts the “area of effect” of the top pair that we already aligned.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-25.png' style='width: 60%'/><font color='gray'><i>We want the 2nd pair to start influcing the output where the first pair deactivates, so we want to adjust the function horizontally.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-26.png' style='width: 60%'/><font color='gray'><i>Flip the 2nd pair’s function segment by flipping the weight to the output neuron.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-27.png' style='width: 60%'/><font color='gray'><i>Use the bottom pair to fix the vertical offset.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-28.png' style='width: 60%'/><font color='gray'><i>Continue the method, begin the activation for the 3rd pair of hidden layer neurons when we wish the slope to start going down.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-29.png' style='width: 60%'/><font color='gray'><i>Repeat the process for each section, giving us a final result.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-30.png' style='width: 60%'/><font color='gray'><i>Pass data through, the neuron’s areas of effect come into play – only when both neurons are activated based on input. With input 0.08, the only pairs activated are the top ones, as this is their area of effect.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-31.png' style='width: 60%'/><font color='gray'><i>With input 0.51, the 4th pair of neurons is activated. Even without any of the other weights, with ReLU we can fit the sine wave pretty well.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-32.png' style='width: 60%'/><font color='gray'><i>If we enable all of the weights now and allow a mathematical optimizer to train, we can see even better fitment.</i></font></center>\n",
    "\n",
    "<center><img src='./image/4-33.png' style='width: 60%'/><font color='gray'><i>More neurons can enable more unique areas of effect, we need two or more hidden layers, we need nonlinear activation functions to map nonlinear problems.</i></font></center>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373945f0",
   "metadata": {},
   "source": [
    "## 4.9. ReLU Activation Function Code\n",
    "\n",
    "A simple version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32541022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "output = []\n",
    "for i in inputs:\n",
    "    if i > 0 :\n",
    "        output.append(i)\n",
    "    else :\n",
    "        output.append( 0 )\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5689b9",
   "metadata": {},
   "source": [
    "A more simple version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed6b8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "output = []\n",
    "for i in inputs:\n",
    "    output.append(max(0, i))\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ed597",
   "metadata": {},
   "source": [
    "The new rectified linear activation class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59555415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU :\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward ( self , inputs ):\n",
    "        \n",
    "        # Calculate output values from input\n",
    "        self.output = np.maximum( 0 , inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13483324",
   "metadata": {},
   "source": [
    "Apply this activation function to the dense layer’s outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3564c6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.        ]\n",
      " [0.         0.00011395 0.        ]\n",
      " [0.         0.00031729 0.        ]\n",
      " [0.         0.00052666 0.        ]\n",
      " [0.         0.00071401 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data( samples = 100 , classes = 3 )\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense( 2 , 3 )\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Forward pass through activation func, takes in output from previous layer\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print (activation1.output[: 5 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ecf69",
   "metadata": {},
   "source": [
    "## 4.10. The Softmax Activation Function\n",
    "\n",
    "ReLU is\n",
    "\n",
    "- Unbounded\n",
    "- Not normalize with other units: values can be anything, an output of `[12, 99, 318]` is without context.\n",
    "- Exclusive: each output is independent of the others.\n",
    "\n",
    "We want this model to be a classifier, so we want in activation function meant for classification. One of these is the Softmax activation function.\n",
    "\n",
    "- Can take in non-normalized or uncalibrated inputs and produce a normalized distribution of probabilities for our classes (add up to 1).\n",
    "- The predicted class is associated with the output neuron that return the largest <b>confidence score</b>.\n",
    "\n",
    "The Softmax function is\n",
    "\n",
    "$$\n",
    "S_{i,j}\\ =  \\frac{e^{z_{i,j}}}{\\sum_{l=1}^{L} e^{z_{i,l}}}\n",
    "$$\n",
    "\n",
    "$e = 2.71828182846$ is the exponential growth number.\n",
    "\n",
    "$z_{i,j}$ means a singular output value, the index $i$ means the current sample and the index $j$ means the current output in this sample.\n",
    "\n",
    "<center><img src='./image/4-34.png' style='width: 60%'/><font color='gray'><i>Graph of an exponential function.</i></font></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f80228c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponentiated values:\n",
      "[121.51041751893969, 3.3534846525504487, 10.85906266492961]\n"
     ]
    }
   ],
   "source": [
    "# Values from the previous output in Chapter 2, section 2.6\n",
    "layer_outputs = [ 4.8 , 1.21 , 2.385 ]\n",
    "\n",
    "# e - we use E here to match a common coding style where constants are uppercased\n",
    "E = 2.71828182846     # you can also use math.e\n",
    "\n",
    "# For each value in a vector, calculate the exponential value\n",
    "exp_values = []\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(E ** output)     # ** - power operator in Python\n",
    "\n",
    "print('Exponentiated values:')\n",
    "print(exp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b386a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized exponentiated values:\n",
      "[0.8952826639573506, 0.024708306782070668, 0.08000902926057876]\n",
      "Sum of normalized values: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Now normalize values\n",
    "norm_base = sum(exp_values)\n",
    "norm_values = []\n",
    "\n",
    "for value in exp_values:\n",
    "    norm_values.append(value / norm_base)\n",
    "\n",
    "print('Normalized exponentiated values:')\n",
    "print(norm_values)\n",
    "print('Sum of normalized values:', sum(norm_values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb86e63",
   "metadata": {},
   "source": [
    "We can perform the same set of operations with the use of NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1c7c29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponentiated values:\n",
      "[121.51041752   3.35348465  10.85906266]\n",
      "Normalized exponentiated values:\n",
      "[0.89528266 0.02470831 0.08000903]\n",
      "Sum of normalized values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Values from the earlier previous when we described what a neural network is\n",
    "layer_outputs = [ 4.8 , 1.21 , 2.385 ]\n",
    "\n",
    "# For each value in a vector, calculate the exponential value\n",
    "exp_values = np.exp(layer_outputs)\n",
    "print('Exponentiated values:' )\n",
    "print(exp_values)\n",
    "\n",
    "# Now normalize values\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "print('Normalized exponentiated values:' )\n",
    "print(norm_values)\n",
    "print('Sum of normalized values:' , np.sum(norm_values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d836db6",
   "metadata": {},
   "source": [
    "The results are similar, but faster to calculate and the code is easier to read with NumPy.\n",
    "\n",
    "To train in batches, we need to convert this functionality to accept layer outputs in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27764a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Get unnormalized probabilities (max subtraction to avoid “dead neurons” and “exploding”)\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b8e63",
   "metadata": {},
   "source": [
    "Exploding phenomena: it doesn’t take a very large number (a mere 1000) to cause an overflow error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "714fbbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.718281828459045\n",
      "22026.465794806718\n",
      "2.6881171418161356e+43\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_49456\\915342352.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  print(np.exp(1000))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.exp(1))\n",
    "print(np.exp(10))\n",
    "print(np.exp(100))\n",
    "print(np.exp(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42df9756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.exp(-np.inf), np.exp(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd9cdc1",
   "metadata": {},
   "source": [
    "We can subtract any value from all of the inputs, and it will not change the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09ae2a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09003057 0.24472847 0.66524096]]\n"
     ]
    }
   ],
   "source": [
    "softmax = Activation_Softmax()\n",
    "softmax.forward([[1, 2, 3]])\n",
    "print (softmax.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b1ca10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09003057 0.24472847 0.66524096]]\n"
     ]
    }
   ],
   "source": [
    "softmax.forward([[ - 2 , - 1 , 0 ]]) # subtracted 3 - max from the list\n",
    "print (softmax.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c6a3fc",
   "metadata": {},
   "source": [
    "If we divide the layer’s output data `[1, 2, 3]` by 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "631355c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18632372 0.30719589 0.50648039]]\n"
     ]
    }
   ],
   "source": [
    "softmax.forward([[0.5, 1, 1.5]])\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94a5349",
   "metadata": {},
   "source": [
    "The output confidences changed due to the nonlinearity nature of the exponentiation. That’s why we need to scale all of the input data to a NN in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82a29d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]]\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Make a forward pass through activation function it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Make a forward pass through second Dense layer it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass through activation function it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[: 5 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde1386e",
   "metadata": {},
   "source": [
    "The distribution of predictions is almost equal, as each of the samples has 0.33 predictions for each class. This results from the random initialization of weights (a draw from the normal distribution) and zeroed biases.\n",
    "\n",
    "`argmax` returns the indices of the maximum values along an axis. The confidence score can be as important as the class prediction itself. `argmax` of `[0.22, 0.6, 0.18]` is the same as the argmax for `[0.32, 0.36, 0.32]`, but 60% confidence is much better than a 36% confidence.\n",
    "\n",
    "The full code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a0b2805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities, axis 0 means row wise and 1 means columns wise\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Make a forward pass through activation function it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Make a forward pass through second Dense layer it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass through activation function it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b5836a",
   "metadata": {},
   "source": [
    "Although neurons are interconnected, they each have their respective weights and biases and are not “normalized” with each other.\n",
    "\n",
    "Our example model is currently random. We need a way to calculate how wrong the NN is at current predictions and begin adjusting weights and biases to decrease error over time.\n",
    "\n",
    "Our next step is to quantify how wrong the model is through what’s defined as a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b34364e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
