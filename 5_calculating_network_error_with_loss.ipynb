{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fd432f",
   "metadata": {},
   "source": [
    "# Chapter 5: Calculating Network Error with Loss\n",
    "\n",
    "The loss function or the cost function is used to quantify how wrong a model is.\n",
    "\n",
    "The output of a NN is confidence, more confidence in the correct answer is better.\n",
    "\n",
    "We try to increase correct confidence and decrease misplaced confidence.\n",
    "\n",
    "## 5.1. Categorical Cross-Entropy Loss\n",
    "\n",
    "In classification, the Softmax activation on the output layer returns a probability distribution. The predicted distribution ($\\hat y$ or predictions) is compared with the ground-truth probability ( $y$ or targets) using categorical cross-entropy.\n",
    "\n",
    "The categorical cross-entropy of $y$ (actual/desired distribution) and $\\hat y$ (predicted distribution) is:\n",
    "\n",
    "$$\n",
    "L_i = - \\sum_{j} y_{i,j} \\ \\text{log} \\ (\\hat{y}_{i,j})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e855dc01",
   "metadata": {},
   "source": [
    "$L_i$ denotes sample loss value, $i$ is the  $i$-th sample in the set, $j$ is the label/output index, $y$ denotes the target values, and $\\hat y$ denotes the predicted values.\n",
    "\n",
    "In coding, we simplify it further to\n",
    "\n",
    "$$\n",
    "L_i = \\ \\text{log} \\ \\big(  \\hat{y}_{i,k} \\big) \\qquad \\text{where }  k \\text{ is an index of \"true\" probability}\n",
    "$$\n",
    "\n",
    "$L_i$ denotes sample loss value, $i$ is the  $i$-th sample in the set, $k$ is the index of the target label (ground-true label), $y$ denotes the target values, and $\\hat y$ denotes the predicted values.\n",
    "\n",
    "Consider a Softmax output of `[0.7, 0.1, 0.2]` and targets of `[1, 0, 0]`, we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L_i & = - \\sum_{j} y_{i,j} \\ \\text{log} \\ (\\hat{y}_{i,j}) = - \\Big( 1 \\cdot \\text{log} \\ (0.7) +0 \\cdot \\text{log} \\ (0.1) + 0 \\cdot \\text{log} \\ (0.2) \\Big)\\\\\n",
    "& = -\\Big( -0.35667494393873245 +0 + 0 \\Big) = 0.35667494393873245 \\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74368543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n",
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# An example output from the output layer of the neural network\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "\n",
    "# Ground truth\n",
    "target_output = [1, 0, 0]\n",
    "loss = - (math.log(softmax_output[0]) * target_output[0] +\n",
    "          math.log(softmax_output[1]) * target_output[1] +\n",
    "          math.log(softmax_output[2]) * target_output[2])\n",
    "print (loss)\n",
    "\n",
    "loss = - math.log(softmax_output[0])\n",
    "print (loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe9d4ea",
   "metadata": {},
   "source": [
    "Update our process to work on batches of softmax output distribution and make the negative log calculation dynamic to the target index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afe6efa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "# Probabilities for 3 samples\n",
    "softmax_outputs = [[0.7, 0.1, 0.2], [0.1, 0.5, 0.4], [0.02, 0.9, 0.08]]\n",
    "class_targets = [0, 1, 1]    # numbers contained are the correct class numbers\n",
    "for targ_idx, distribution in zip (class_targets, softmax_outputs):\n",
    "    print (distribution[targ_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fbadf6",
   "metadata": {},
   "source": [
    "This can be further simplified using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1c8ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2], [0.1, 0.5, 0.4], [0.02, 0.9, 0.08]])\n",
    "class_targets = [0, 1, 1]\n",
    "print (softmax_outputs[[0, 1, 2], class_targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b28faaf",
   "metadata": {},
   "source": [
    "This returns a list of the confidences at the target indices for each of the samples.\n",
    "\n",
    "We can use a `range()` instead of type each value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe87126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "print(softmax_outputs[range(len(softmax_outputs)), class_targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b664b1",
   "metadata": {},
   "source": [
    "Now apply the negative log to this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3c05018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(softmax_outputs[range(len(softmax_outputs)), class_targets]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3011184d",
   "metadata": {},
   "source": [
    "Average loss per batch to have an idea about how our model is doing during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b603509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "neg_log = - np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
    "average_loss = np.mean(neg_log)\n",
    "print (average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9354b41",
   "metadata": {},
   "source": [
    "Targets can be one-hot encoded where the correct labelâ€™s position is filled with 1, all others are zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ca4a7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2], [0.1, 0.5, 0.4], [0.02, 0.9, 0.08]])\n",
    "class_targets = np.array([[1, 0, 0], [0, 1, 0], [0, 1, 0]])\n",
    "\n",
    "# Probabilities for target values - only if categorical labels\n",
    "if len (class_targets.shape) == 1:   # for [0,1,1]\n",
    "    correct_confidences = softmax_outputs[ range ( len (softmax_outputs)), class_targets]\n",
    "\n",
    "# Mask values - only for one-hot encoded labels\n",
    "elif len (class_targets.shape) == 2:\n",
    "    correct_confidences = np.sum(softmax_outputs * class_targets, axis = 1)\n",
    "\n",
    "# Losses\n",
    "neg_log = - np.log(correct_confidences)\n",
    "average_losses = np.mean(neg_log)\n",
    "\n",
    "print(average_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43d5e5",
   "metadata": {},
   "source": [
    "## 5.2. The Categorical Cross-Entropy Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e08e1b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "# Common loss class\n",
    "class Loss :\n",
    "    # Calculates the data and regularization losses given model output and ground truth values\n",
    "    def calculate ( self , output , y ):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Probabilities for target values - only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[ range(samples), y_true]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum( y_pred_clipped*y_true, axis=1)\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(softmax_outputs, class_targets)\n",
    "print (loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21185543",
   "metadata": {},
   "source": [
    "## 5.3. Combining everything up to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdf45a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "Hey Um sample losees:  [1.0986123 1.0986127 1.0986136 1.0986145 1.0986153 1.0986137 1.0986168\n",
      " 1.098617  1.0986178 1.0986192 1.0986199 1.098619  1.0986207 1.0986221\n",
      " 1.098622  1.0986223 1.0986185 1.0986241 1.0986226 1.098626  1.098626\n",
      " 1.0986195 1.098617  1.0986255 1.0986354 1.0986278 1.0986179 1.0986184\n",
      " 1.0986398 1.0986419 1.0986304 1.098637  1.0986211 1.0986254 1.0986376\n",
      " 1.0986471 1.0986526 1.098653  1.0986507 1.098654  1.0986497 1.0986483\n",
      " 1.0986478 1.0986323 1.0986618 1.098662  1.098664  1.0986414 1.0986664\n",
      " 1.0986533 1.0986621 1.0986348 1.0986497 1.0986599 1.0986315 1.0986141\n",
      " 1.0986186 1.0986125 1.0986273 1.0986137 1.0986171 1.098616  1.0986123\n",
      " 1.0986311 1.0986419 1.0986344 1.0986123 1.0986576 1.0986377 1.0986589\n",
      " 1.0986654 1.0986648 1.0986633 1.0986542 1.0986674 1.0986677 1.0986686\n",
      " 1.0986708 1.0986696 1.0986639 1.0986731 1.0986443 1.0986497 1.0986748\n",
      " 1.0986497 1.0986748 1.0986547 1.0986434 1.0986588 1.0986766 1.0986344\n",
      " 1.0986977 1.0986686 1.0987039 1.0986919 1.098709  1.0986958 1.0987172\n",
      " 1.0987127 1.098722  1.0986123 1.0986128 1.0986135 1.0986153 1.0986153\n",
      " 1.0986152 1.0986183 1.0986192 1.0986147 1.0986161 1.0986137 1.0986179\n",
      " 1.0986241 1.098613  1.0986227 1.0986149 1.0986123 1.0986197 1.0986123\n",
      " 1.0986149 1.0986127 1.0986228 1.0986152 1.0986241 1.0986125 1.098617\n",
      " 1.0986165 1.0986314 1.0986289 1.0986291 1.0986217 1.098633  1.0986272\n",
      " 1.098634  1.0986317 1.0986371 1.0986376 1.0986385 1.0986384 1.0986383\n",
      " 1.0986378 1.0986364 1.0986375 1.0986311 1.0986297 1.0986218 1.0986421\n",
      " 1.0986165 1.0986434 1.0986321 1.0986229 1.0986485 1.098628  1.0986263\n",
      " 1.0986216 1.098616  1.0986595 1.0986164 1.098617  1.0986412 1.0986454\n",
      " 1.098671  1.098675  1.0986711 1.0986497 1.0986769 1.098666  1.098659\n",
      " 1.0986668 1.0986786 1.0986652 1.0986745 1.0986578 1.0986841 1.0986865\n",
      " 1.0986457 1.0986809 1.0986754 1.0986645 1.0986422 1.0986489 1.0986755\n",
      " 1.0986456 1.0986873 1.0986316 1.09866   1.0986328 1.0986334 1.0986155\n",
      " 1.0986733 1.0986123 1.0986614 1.0986596 1.0986346 1.0986767 1.0986708\n",
      " 1.0986807 1.0986805 1.0986531 1.0986693 1.0986123 1.0986109 1.09861\n",
      " 1.0986086 1.0986078 1.0986054 1.0986048 1.0986024 1.0986047 1.098603\n",
      " 1.098609  1.0986058 1.0986083 1.0986017 1.0985988 1.0986042 1.0986068\n",
      " 1.0985962 1.0985739 1.0985948 1.0986065 1.0985909 1.0985881 1.0985681\n",
      " 1.098602  1.0985644 1.0985594 1.0985554 1.0985595 1.0985518 1.0985633\n",
      " 1.0985466 1.098544  1.0985512 1.0985405 1.0985653 1.098593  1.0985372\n",
      " 1.0985689 1.0985835 1.098547  1.0985583 1.0985734 1.0985739 1.0985808\n",
      " 1.0985378 1.0985814 1.0985769 1.098592  1.0985706 1.0985731 1.0986037\n",
      " 1.0985707 1.0985788 1.0985432 1.0985382 1.0985696 1.0985863 1.0985857\n",
      " 1.0985255 1.0985543 1.0985299 1.0985267 1.0985194 1.0985484 1.098522\n",
      " 1.0985324 1.0985252 1.0985198 1.0985336 1.0985485 1.0985223 1.0985363\n",
      " 1.0985707 1.098536  1.0985528 1.0985816 1.0985836 1.098514  1.0984505\n",
      " 1.098563  1.0984706 1.0985769 1.0984368 1.0984722 1.098446  1.0985355\n",
      " 1.098427  1.0984247 1.0984327 1.098446  1.0984297 1.0985248 1.0984324\n",
      " 1.0984129 1.0984786 1.0984266 1.0984656 1.0984188 1.098476 ]\n",
      "Loss: 1.0986104\n",
      "Acc: 0.34\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        #print(\"Hey Um sample losees: \", sample_losses)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Probabilities for target values - only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[ range(samples), y_true]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum( y_pred_clipped*y_true, axis=1)\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 3 input features (as we take output previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "# Perform a forward pass through activation function takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Perform a forward pass through second Dense layer it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Perform a forward pass through activation function it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])\n",
    "# Perform a forward pass through loss function it takes the output of second dense layer here and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "# Print loss value\n",
    "print('Loss:', loss)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "predictions = np.argmax(activation2.output, axis = 1 )\n",
    "if len (y.shape) == 2 :\n",
    "    y = np.argmax(y, axis = 1 )\n",
    "accuracy = np.mean(predictions == y)\n",
    "# Print accuracy\n",
    "print ('Acc:' , accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91908ac",
   "metadata": {},
   "source": [
    "## 5.4. Accuracy Calculation \n",
    "\n",
    "Loss is a useful metric for optimizing a model.\n",
    "\n",
    "Accuracy is commonly used to describes how often the largest confidence is the correct class in terms of a fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7008263a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Probabilities of 3 samples\n",
    "softmax_outputs = np.array([[ 0.7 , 0.2 , 0.1 ],\n",
    "                            [ 0.5 , 0.1 , 0.4 ],\n",
    "                            [ 0.02 , 0.9 , 0.08 ]])\n",
    "\n",
    "# Target (ground-truth) labels for 3 samples\n",
    "class_targets = np.array([ 0 , 1 , 1 ])\n",
    "\n",
    "# Calculate values along second axis (axis of index 1)\n",
    "predictions = np.argmax(softmax_outputs, axis = 1 )\n",
    "\n",
    "# If targets are one-hot encoded - convert them\n",
    "if len (class_targets.shape) == 2 :\n",
    "    class_targets = np.argmax(class_targets, axis = 1 )\n",
    "\n",
    "# True evaluates to 1; False to 0\n",
    "accuracy = np.mean(predictions == class_targets)\n",
    "print ( 'acc:' , accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae33e5d9",
   "metadata": {},
   "source": [
    "We performed a forward pass through our network and calculated the metrics to signal if the model is performing poorly, we will embark on optimization in the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0803645c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnfs",
   "language": "python",
   "name": "nnfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
