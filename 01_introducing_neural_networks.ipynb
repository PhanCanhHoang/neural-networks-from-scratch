{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Chapter 1: Introducing Neural Networks\n",
    "\n",
    "Artificial Neural Networks or Neural Networks ('artificial' has been dropped recently) are a shared part of ML and DL.\n",
    "\n",
    "A deep NN has two or more hidden layers. \n",
    "\n",
    "Most NNs in use are a form of deep learning.\n",
    "\n",
    "<center><img src='./image/1-1-ai-complete-graph.jpeg' style='width: 80%'/></center>\n",
    "\n",
    "## 1.1. A Brief History\n",
    "\n",
    "In the 1940s, NNs were conceived.\n",
    "\n",
    "In the 1960s, the concept of backpropagation came, then people know how to train them.\n",
    "\n",
    "In 2010, NNs started winning competitions and get much attention than before.\n",
    "\n",
    "Since 2010, NNs have been on a meteoric rise as their magical ability to solve problems previously deemed unsolvable (i.e., image captioning, language translation, audio and video synthesis, and more).\n",
    "\n",
    "Currently, NNs are the primary solution to most competitions and technological challenges like self-driving cars, calculating risk, detecting fraud, early cancer detection,…\n",
    "\n",
    "## 1.2. What is a Neural Network?\n",
    "\n",
    "ANNs are inspired by the organic brain, translated to the computer.\n",
    "\n",
    "ANNs have neurons, activations, and interconnectivities.\n",
    "\n",
    "NNs are considered “black boxes” between inputs and outputs.\n",
    "\n",
    "<center><img src='./image/1-2-example-nn.png' style='width: 60%'/></center>\n",
    "\n",
    "Each connection between neurons has a weight associated with it. Weights are multiplied by corresponding input values. These multiplications flow into the neuron and are summed before being added with a bias. Weights and biases are trainable or tunable.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "output & = weight \\cdot input + bias \\\\\n",
    "y & = a \\cdot x + b\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Adjusting the weight will impact the slope of the function.\n",
    "\n",
    "<center><img src='./image/1-3-adjust-weight.png' style='width: 60%'/></center>\n",
    "\n",
    "<center><img src='./image/1-4-adjust-weight.png' style='width: 60%'/></center>\n",
    "\n",
    "<center><img src='./image/1-5-adjust-weight.png' style='width: 60%'/></center>\n",
    "\n",
    "The bias offsets the overall function.\n",
    "\n",
    "<center><img src='./image/1-6-adjust-bias.png' style='width: 60%'/></center>\n",
    "\n",
    "<center><img src='./image/1-7-adjust-bias.png' style='width: 60%'/></center>\n",
    "\n",
    "Weights and biases impact the outputs of neurons in slightly different ways.\n",
    "\n",
    "Then, an activation function is applied to the output.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "output & = \\sum (weight \\cdot input) + bias \\\\\n",
    "output & = activation (output)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "When a step function that mimics a neuron in the brain (i.e., “firing” or not, on-off switch) is used as an activation function:\n",
    "- If its output is greater than 0, the neuron fires (it would output 1).\n",
    "- If its output is less than 0, the neuron does not fire and would pass along a 0.\n",
    "\n",
    "NNs of today tend to use more informative activation functions (rather than a step function), such as Rectified Linear (ReLU) activation function.\n",
    "\n",
    "Example basic neural networks:\n",
    "\n",
    "<center><img src='./image/1-8-basic-nn.png' style='width: 60%'/></center>\n",
    "\n",
    "The input layer represents the actual input data (i.e., pixel values from an image, temperature, …)\n",
    "\n",
    "- The data can be “raw”, should be preprocessed like normalization and scaling. \n",
    "- The input needs to be in numeric form.\n",
    "\n",
    "The output layer is whatever the NN returns.\n",
    "- In classification, the class of the input is predicted, the output layer has as many neurons as the training dataset has classes. But can also have a single output neuron for binary (two classes) classification.\n",
    "\n",
    "For example, our goal is to classify a collection of pictures as a “dog” or “cat”:\n",
    "\n",
    "- The output layer has two neurons, one associated with “dog” and one associated with “cat”.\n",
    "- Can have just a single output neuron that is “dog” or “not dog”.\n",
    "\n",
    "<center><img src='./image/1-9-nn-dog.png' style='width: 60%'/></center>\n",
    "\n",
    "<center><img src='./image/1-10-nn-dog.png' style='width: 60%'/></center>\n",
    "\n",
    "The math involved makes NNs appear challenging and how scary it can sometimes look.\n",
    "\n",
    "The full formula for the forward pass of an example NN model:\n",
    "\n",
    "<center><img src='./image/1-11-loss-function.png' style='width: 90%'/></center>\n",
    "\n",
    "This function can also be represented in nested python functions like:\n",
    "\n",
    "<center><img src='./image/1-12-loss-function-code.png' style='width: 80%'/></center>\n",
    "\n",
    "High school algebra is enough to understand:\n",
    "- A log function\n",
    "- A sum operation\n",
    "- An exponentiating operation\n",
    "- A dot product\n",
    "- Transpose\n",
    "\n",
    "A typical NN has thousands or even up to millions of adjustable parameters (weights and biases).\n",
    "\n",
    "NNs act as enormous functions with vast numbers of parameters.\n",
    "\n",
    "Finding the combination of parameter (weight and bias) values is the challenging part.\n",
    "\n",
    "The end goal for NNs is to adjust their weights and biases (the parameters), so they produce the desired output for unseen data.\n",
    "\n",
    "A major issue in supervised learning is overfitting, where the algorithm doesn’t understand underlying input-output dependencies, just basically “memorizes” the training data.\n",
    "\n",
    "The goal of NN is generalization, that can be obtained when separating the data into training data and validation data.\n",
    "\n",
    "Weights and biases are adjusted based on the error/loss presenting how “wrong” the algorithm in NN predicting the output.\n",
    "\n",
    "NNs can be used for regression (predict a scalar, singular, value), clustering (assigned unstructured data into groups), and many other tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
